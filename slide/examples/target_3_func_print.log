[2023-07-20 06:40:08,337] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:08,337] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:08,337] torch._dynamo.eval_frame: [DEBUG] skipping helper /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:08,337] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:08,337] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:08,337] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:08,337] torch._utils_internal: [INFO] dynamo _convert_frame_assert._compile: {'co_name': 'func', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 31, 'cache_size': 0}
[2023-07-20 06:40:08,338] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func slide/examples/target_3_func_print.py:31
[2023-07-20 06:40:08,338] torch.fx.experimental.symbolic_shapes: [INFO] 0.0: create_env
[2023-07-20 06:40:08,338] torch._subclasses.fake_tensor: [DEBUG] create_mode 0x7fe17d347be0
TRACE starts_line func slide/examples/target_3_func_print.py:31
    @dynamo.optimize(my_compiler)
[2023-07-20 06:40:08,364] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['a'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:08,364] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b'] (10,) [<DimDynamic.STATIC: 2>] [None]
TRACE starts_line func slide/examples/target_3_func_print.py:33
        d = func0(a, b)
[2023-07-20 06:40:08,365] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL func0 []
[2023-07-20 06:40:08,365] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [UserFunctionVariable()]
[2023-07-20 06:40:08,365] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [UserFunctionVariable(), TensorVariable()]
[2023-07-20 06:40:08,365] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [UserFunctionVariable(), TensorVariable(), TensorVariable()]
[2023-07-20 06:40:08,366] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object func0 at 0x7fe1b45aed40, file "slide/examples/target_3_func_print.py", line 26>
TRACE starts_line func0 slide/examples/target_3_func_print.py:26 (inline depth: 1)
    def func0(a, b):
TRACE starts_line func0 slide/examples/target_3_func_print.py:27 (inline depth: 1)
        c = func1(a, b)
[2023-07-20 06:40:08,366] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL func1 []
[2023-07-20 06:40:08,366] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [UserFunctionVariable()]
[2023-07-20 06:40:08,366] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [UserFunctionVariable(), TensorVariable()]
[2023-07-20 06:40:08,366] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [UserFunctionVariable(), TensorVariable(), TensorVariable()]
[2023-07-20 06:40:08,366] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object func1 at 0x7fe1b45ae240, file "slide/examples/target_3_func_print.py", line 20>
TRACE starts_line func1 slide/examples/target_3_func_print.py:20 (inline depth: 2)
    def func1(a0, b0):
TRACE starts_line func1 slide/examples/target_3_func_print.py:21 (inline depth: 2)
        print("func1")
[2023-07-20 06:40:08,366] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL print []
[2023-07-20 06:40:08,366] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST func1 [BuiltinVariable(print)]
[2023-07-20 06:40:08,367] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [BuiltinVariable(print), ConstantVariable(str)]
[2023-07-20 06:40:08,367] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object func1 at 0x7fe1b45ae240, file "slide/examples/target_3_func_print.py", line 20>
[2023-07-20 06:40:08,367] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes
[2023-07-20 06:40:08,367] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object func0 at 0x7fe1b45aed40, file "slide/examples/target_3_func_print.py", line 26>
[2023-07-20 06:40:08,367] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes
[2023-07-20 06:40:08,367] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile
Graph break: call_function BuiltinVariable(print) [ConstantVariable(str)] {} from user code at   File "slide/examples/target_3_func_print.py", line 33, in func
    d = func0(a, b)
  File "slide/examples/target_3_func_print.py", line 27, in func0
    c = func1(a, b)
  File "slide/examples/target_3_func_print.py", line 21, in func1
    print("func1")

[2023-07-20 06:40:08,367] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes
[2023-07-20 06:40:08,368] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='call_function BuiltinVariable(print) [ConstantVariable(str)] {}', user_stack=[<FrameSummary file slide/examples/target_3_func_print.py, line 33 in func>, <FrameSummary file slide/examples/target_3_func_print.py, line 27 in func0>, <FrameSummary file slide/examples/target_3_func_print.py, line 21 in func1>], graph_break=True)
ORIGINAL BYTECODE func slide/examples/target_3_func_print.py line 31 
 33           0 LOAD_GLOBAL              0 (func0)
              2 LOAD_FAST                0 (a)
              4 LOAD_FAST                1 (b)
              6 CALL_FUNCTION            2
              8 STORE_FAST               2 (d)

 34          10 LOAD_FAST                2 (d)
             12 RETURN_VALUE


MODIFIED BYTECODE func slide/examples/target_3_func_print.py line 31 
 31           0 LOAD_GLOBAL              0 (func0)
              2 LOAD_FAST                0 (a)
              4 LOAD_FAST                1 (b)

 33           6 CALL_FUNCTION            2
              8 LOAD_GLOBAL              1 (__resume_at_8_0)
             10 ROT_TWO
             12 CALL_FUNCTION            1
             14 RETURN_VALUE


[2023-07-20 06:40:08,390] torch.fx.experimental.symbolic_shapes: [INFO] 0.0: produce_guards
[2023-07-20 06:40:08,390] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['a'].size()[0] == 10
[2023-07-20 06:40:08,390] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['a'].stride()[0] == 1
[2023-07-20 06:40:08,390] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['a'].storage_offset() == 0
[2023-07-20 06:40:08,390] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['b'].size()[0] == 10
[2023-07-20 06:40:08,390] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['b'].stride()[0] == 1
[2023-07-20 06:40:08,390] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['b'].storage_offset() == 0
[2023-07-20 06:40:08,390] torch._utils_internal: [INFO] dynamic produce_guards: {'co_name': 'func', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 31, 'num_guards': 0, 'free_symbols': 0}
GUARDS:
  hasattr(L['a'], '_dynamo_dynamic_indices') == False
  hasattr(L['b'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:08,392] torch._utils_internal: [INFO] dynamo _convert_frame_assert._compile: {'co_name': 'func0', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 26, 'cache_size': 0}
[2023-07-20 06:40:08,393] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func0 slide/examples/target_3_func_print.py:26
[2023-07-20 06:40:08,393] torch.fx.experimental.symbolic_shapes: [INFO] 1.0: create_env
[2023-07-20 06:40:08,393] torch._subclasses.fake_tensor: [DEBUG] create_mode 0x7fe17d0b4eb0
TRACE starts_line func0 slide/examples/target_3_func_print.py:26
    def func0(a, b):
[2023-07-20 06:40:08,393] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['a'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:08,394] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b'] (10,) [<DimDynamic.STATIC: 2>] [None]
TRACE starts_line func0 slide/examples/target_3_func_print.py:27
        c = func1(a, b)
[2023-07-20 06:40:08,394] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL func1 []
[2023-07-20 06:40:08,394] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [UserFunctionVariable()]
[2023-07-20 06:40:08,394] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [UserFunctionVariable(), TensorVariable()]
[2023-07-20 06:40:08,394] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [UserFunctionVariable(), TensorVariable(), TensorVariable()]
[2023-07-20 06:40:08,394] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object func1 at 0x7fe1b45ae240, file "slide/examples/target_3_func_print.py", line 20>
TRACE starts_line func1 slide/examples/target_3_func_print.py:20 (inline depth: 1)
    def func1(a0, b0):
TRACE starts_line func1 slide/examples/target_3_func_print.py:21 (inline depth: 1)
        print("func1")
[2023-07-20 06:40:08,395] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL print []
[2023-07-20 06:40:08,395] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST func1 [BuiltinVariable(print)]
[2023-07-20 06:40:08,395] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [BuiltinVariable(print), ConstantVariable(str)]
[2023-07-20 06:40:08,395] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object func1 at 0x7fe1b45ae240, file "slide/examples/target_3_func_print.py", line 20>
[2023-07-20 06:40:08,395] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes
[2023-07-20 06:40:08,395] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile
[2023-07-20 06:40:08,395] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes
[2023-07-20 06:40:08,395] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='call_function BuiltinVariable(print) [ConstantVariable(str)] {}', user_stack=[<FrameSummary file slide/examples/target_3_func_print.py, line 27 in func0>, <FrameSummary file slide/examples/target_3_func_print.py, line 21 in func1>], graph_break=True)
ORIGINAL BYTECODE func0 slide/examples/target_3_func_print.py line 26 
 27           0 LOAD_GLOBAL              0 (func1)
              2 LOAD_FAST                0 (a)
              4 LOAD_FAST                1 (b)
              6 CALL_FUNCTION            2
              8 STORE_FAST               2 (c)

 28          10 LOAD_FAST                2 (c)
             12 RETURN_VALUE


MODIFIED BYTECODE func0 slide/examples/target_3_func_print.py line 26 
 26           0 LOAD_GLOBAL              0 (func1)
              2 LOAD_FAST                0 (a)
              4 LOAD_FAST                1 (b)

 27           6 CALL_FUNCTION            2
              8 LOAD_GLOBAL              1 (__resume_at_8_1)
             10 ROT_TWO
             12 CALL_FUNCTION            1
             14 RETURN_VALUE


[2023-07-20 06:40:08,396] torch.fx.experimental.symbolic_shapes: [INFO] 1.0: produce_guards
[2023-07-20 06:40:08,396] torch.fx.experimental.symbolic_shapes: [DEBUG] 1.0: Skipping guard L['a'].size()[0] == 10
[2023-07-20 06:40:08,396] torch.fx.experimental.symbolic_shapes: [DEBUG] 1.0: Skipping guard L['a'].stride()[0] == 1
[2023-07-20 06:40:08,396] torch.fx.experimental.symbolic_shapes: [DEBUG] 1.0: Skipping guard L['a'].storage_offset() == 0
[2023-07-20 06:40:08,396] torch.fx.experimental.symbolic_shapes: [DEBUG] 1.0: Skipping guard L['b'].size()[0] == 10
[2023-07-20 06:40:08,396] torch.fx.experimental.symbolic_shapes: [DEBUG] 1.0: Skipping guard L['b'].stride()[0] == 1
[2023-07-20 06:40:08,396] torch.fx.experimental.symbolic_shapes: [DEBUG] 1.0: Skipping guard L['b'].storage_offset() == 0
[2023-07-20 06:40:08,396] torch._utils_internal: [INFO] dynamic produce_guards: {'co_name': 'func0', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 26, 'num_guards': 0, 'free_symbols': 0}
GUARDS:
  hasattr(L['a'], '_dynamo_dynamic_indices') == False
  hasattr(L['b'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:08,397] torch._utils_internal: [INFO] dynamo _convert_frame_assert._compile: {'co_name': 'func1', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 20, 'cache_size': 0}
[2023-07-20 06:40:08,398] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func1 slide/examples/target_3_func_print.py:20
[2023-07-20 06:40:08,398] torch.fx.experimental.symbolic_shapes: [INFO] 2.0: create_env
[2023-07-20 06:40:08,398] torch._subclasses.fake_tensor: [DEBUG] create_mode 0x7fe17d04bb50
TRACE starts_line func1 slide/examples/target_3_func_print.py:20
    def func1(a0, b0):
[2023-07-20 06:40:08,398] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['a0'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:08,398] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b0'] (10,) [<DimDynamic.STATIC: 2>] [None]
TRACE starts_line func1 slide/examples/target_3_func_print.py:21
        print("func1")
[2023-07-20 06:40:08,399] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL print []
[2023-07-20 06:40:08,399] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST func1 [BuiltinVariable(print)]
[2023-07-20 06:40:08,399] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [BuiltinVariable(print), ConstantVariable(str)]
[2023-07-20 06:40:08,399] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile
[2023-07-20 06:40:08,399] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes
[2023-07-20 06:40:08,399] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='call_function BuiltinVariable(print) [ConstantVariable(str)] {}', user_stack=[<FrameSummary file slide/examples/target_3_func_print.py, line 21 in func1>], graph_break=True)
ORIGINAL BYTECODE func1 slide/examples/target_3_func_print.py line 20 
 21           0 LOAD_GLOBAL              0 (print)
              2 LOAD_CONST               1 ('func1')
              4 CALL_FUNCTION            1
              6 POP_TOP

 22           8 LOAD_FAST                0 (a0)
             10 LOAD_FAST                1 (b0)
             12 BINARY_ADD
             14 STORE_FAST               2 (c)

 23          16 LOAD_FAST                2 (c)
             18 RETURN_VALUE


MODIFIED BYTECODE func1 slide/examples/target_3_func_print.py line 20 
 20           0 LOAD_GLOBAL              0 (print)
              2 LOAD_CONST               1 ('func1')

 21           4 CALL_FUNCTION            1
              6 LOAD_GLOBAL              1 (__resume_at_6_2)
              8 ROT_TWO
             10 LOAD_FAST                0 (a0)
             12 LOAD_FAST                1 (b0)
             14 CALL_FUNCTION            3
             16 RETURN_VALUE


[2023-07-20 06:40:08,400] torch.fx.experimental.symbolic_shapes: [INFO] 2.0: produce_guards
[2023-07-20 06:40:08,400] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['a0'].size()[0] == 10
[2023-07-20 06:40:08,400] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['a0'].stride()[0] == 1
[2023-07-20 06:40:08,400] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['a0'].storage_offset() == 0
[2023-07-20 06:40:08,400] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['b0'].size()[0] == 10
[2023-07-20 06:40:08,400] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['b0'].stride()[0] == 1
[2023-07-20 06:40:08,400] torch.fx.experimental.symbolic_shapes: [DEBUG] 2.0: Skipping guard L['b0'].storage_offset() == 0
[2023-07-20 06:40:08,400] torch._utils_internal: [INFO] dynamic produce_guards: {'co_name': 'func1', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 20, 'num_guards': 0, 'free_symbols': 0}
GUARDS:
  hasattr(L['a0'], '_dynamo_dynamic_indices') == False
  hasattr(L['b0'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:08,401] torch._utils_internal: [INFO] dynamo _convert_frame_assert._compile: {'co_name': '<resume in func1>', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 21, 'cache_size': 0}
[2023-07-20 06:40:08,402] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in func1> slide/examples/target_3_func_print.py:21
[2023-07-20 06:40:08,402] torch.fx.experimental.symbolic_shapes: [INFO] 3.0: create_env
[2023-07-20 06:40:08,402] torch._subclasses.fake_tensor: [DEBUG] create_mode 0x7fe17d05efd0
TRACE starts_line <resume in func1> slide/examples/target_3_func_print.py:21
        print("func1")
[2023-07-20 06:40:08,402] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['a0'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:08,402] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b0'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:08,403] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []
[2023-07-20 06:40:08,403] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 10 [ConstantVariable(NoneType)]
[2023-07-20 06:40:08,403] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [ConstantVariable(NoneType)]
TRACE starts_line <resume in func1> slide/examples/target_3_func_print.py:22
        c = a0 + b0
[2023-07-20 06:40:08,403] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a0 []
[2023-07-20 06:40:08,403] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b0 [TensorVariable()]
[2023-07-20 06:40:08,403] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:08,403] torch._subclasses.fake_tensor: [DEBUG] FakeTensorMode.__torch_dispatch__: aten.add.Tensor
[2023-07-20 06:40:08,404] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST c [TensorVariable()]
TRACE starts_line <resume in func1> slide/examples/target_3_func_print.py:23
        return c
[2023-07-20 06:40:08,404] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST c []
[2023-07-20 06:40:08,404] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2023-07-20 06:40:08,404] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in func1> (RETURN_VALUE)
[2023-07-20 06:40:08,404] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile
[2023-07-20 06:40:08,404] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file slide/examples/target_3_func_print.py, line 23 in <resume in func1>>], graph_break=False)
TRACED GRAPH
 ===== __compiled_fn_3 =====
 <eval_with_key>.0 class GraphModule(torch.nn.Module):
    def forward(self, L_a0_ : torch.Tensor, L_b0_ : torch.Tensor):
        l_a0_ = L_a0_
        l_b0_ = L_b0_
        
        # File: slide/examples/target_3_func_print.py:22, code: c = a0 + b0
        add = l_a0_ + l_b0_;  l_a0_ = l_b0_ = None
        return (add,)
        

TRACED GRAPH
 __compiled_fn_3 <eval_with_key>.0 opcode         name    target                   args            kwargs
-------------  ------  -----------------------  --------------  --------
placeholder    l_a0_   L_a0_                    ()              {}
placeholder    l_b0_   L_b0_                    ()              {}
call_function  add     <built-in function add>  (l_a0_, l_b0_)  {}
output         output  output                   ((add,),)       {}

TRACED GRAPH TENSOR SIZES
===== __compiled_fn_3 =====
l_a0_: (10,)
l_b0_: (10,)
add: (10,)

[2023-07-20 06:40:08,408] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler
[2023-07-20 06:40:08,408] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler
ORIGINAL BYTECODE <resume in func1> slide/examples/target_3_func_print.py line 21 
 21           0 LOAD_FAST                0 (___stack0)
              2 JUMP_ABSOLUTE           10
              4 LOAD_GLOBAL              0 (print)
              6 LOAD_CONST               1 ('func1')
              8 CALL_FUNCTION            1
        >>   10 POP_TOP

 22          12 LOAD_FAST                1 (a0)
             14 LOAD_FAST                2 (b0)
             16 BINARY_ADD
             18 STORE_FAST               3 (c)

 23          20 LOAD_FAST                3 (c)
             22 RETURN_VALUE


MODIFIED BYTECODE <resume in func1> slide/examples/target_3_func_print.py line 21 
 21           0 LOAD_GLOBAL              1 (__compiled_fn_3)
              2 LOAD_FAST                1 (a0)
              4 LOAD_FAST                2 (b0)
              6 CALL_FUNCTION            2
              8 UNPACK_SEQUENCE          1
             10 RETURN_VALUE


[2023-07-20 06:40:08,409] torch.fx.experimental.symbolic_shapes: [INFO] 3.0: produce_guards
[2023-07-20 06:40:08,409] torch.fx.experimental.symbolic_shapes: [DEBUG] 3.0: Skipping guard L['a0'].size()[0] == 10
[2023-07-20 06:40:08,409] torch.fx.experimental.symbolic_shapes: [DEBUG] 3.0: Skipping guard L['a0'].stride()[0] == 1
[2023-07-20 06:40:08,409] torch.fx.experimental.symbolic_shapes: [DEBUG] 3.0: Skipping guard L['a0'].storage_offset() == 0
[2023-07-20 06:40:08,409] torch.fx.experimental.symbolic_shapes: [DEBUG] 3.0: Skipping guard L['b0'].size()[0] == 10
[2023-07-20 06:40:08,409] torch.fx.experimental.symbolic_shapes: [DEBUG] 3.0: Skipping guard L['b0'].stride()[0] == 1
[2023-07-20 06:40:08,409] torch.fx.experimental.symbolic_shapes: [DEBUG] 3.0: Skipping guard L['b0'].storage_offset() == 0
[2023-07-20 06:40:08,409] torch._utils_internal: [INFO] dynamic produce_guards: {'co_name': '<resume in func1>', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 21, 'num_guards': 0, 'free_symbols': 0}
GUARDS:
  hasattr(L['a0'], '_dynamo_dynamic_indices') == False
  hasattr(L['b0'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:08,410] torch._dynamo.eval_frame: [DEBUG] skipping _fn /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:08,410] torch._dynamo.eval_frame: [DEBUG] skipping nothing /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:08,410] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:08,410] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:08,410] torch._utils_internal: [INFO] dynamo _convert_frame_assert._compile: {'co_name': '<resume in func0>', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 27, 'cache_size': 0}
[2023-07-20 06:40:08,410] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in func0> slide/examples/target_3_func_print.py:27
[2023-07-20 06:40:08,410] torch.fx.experimental.symbolic_shapes: [INFO] 4.0: create_env
[2023-07-20 06:40:08,411] torch._subclasses.fake_tensor: [DEBUG] create_mode 0x7fe17d051c10
TRACE starts_line <resume in func0> slide/examples/target_3_func_print.py:27
        c = func1(a, b)
[2023-07-20 06:40:08,411] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['___stack0'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:08,411] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []
[2023-07-20 06:40:08,411] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 12 [TensorVariable()]
[2023-07-20 06:40:08,411] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST c [TensorVariable()]
TRACE starts_line <resume in func0> slide/examples/target_3_func_print.py:28
        return c
[2023-07-20 06:40:08,411] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST c []
[2023-07-20 06:40:08,411] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2023-07-20 06:40:08,411] torch._dynamo.convert_frame: [DEBUG] Skipping frame because no content in function call <resume in func0>                     slide/examples/target_3_func_print.py 27
[2023-07-20 06:40:08,411] torch._utils_internal: [INFO] dynamo _convert_frame_assert._compile: {'co_name': '<resume in func>', 'co_filename': 'slide/examples/target_3_func_print.py', 'co_firstlineno': 33, 'cache_size': 0}
[2023-07-20 06:40:08,411] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in func> slide/examples/target_3_func_print.py:33
[2023-07-20 06:40:08,412] torch.fx.experimental.symbolic_shapes: [INFO] 5.0: create_env
[2023-07-20 06:40:08,412] torch._subclasses.fake_tensor: [DEBUG] create_mode 0x7fe17cf894f0
TRACE starts_line <resume in func> slide/examples/target_3_func_print.py:33
        d = func0(a, b)
[2023-07-20 06:40:08,412] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['___stack0'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:08,412] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []
[2023-07-20 06:40:08,412] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 12 [TensorVariable()]
[2023-07-20 06:40:08,412] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST d [TensorVariable()]
TRACE starts_line <resume in func> slide/examples/target_3_func_print.py:34
        return d
[2023-07-20 06:40:08,412] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST d []
[2023-07-20 06:40:08,412] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2023-07-20 06:40:08,412] torch._dynamo.convert_frame: [DEBUG] Skipping frame because no content in function call <resume in func>                     slide/examples/target_3_func_print.py 33
func1
my_compiler() called with FX graph:
[2023-07-20 06:40:08,412] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[2023-07-20 06:40:08,412] torch._dynamo.utils: [INFO] Function                          Runtimes (s)
[2023-07-20 06:40:08,412] torch._dynamo.utils: [INFO] ------------------------------  --------------
[2023-07-20 06:40:08,412] torch._dynamo.utils: [INFO] _compile                                0.0739
[2023-07-20 06:40:08,412] torch._dynamo.utils: [INFO] OutputGraph.call_user_compiler          0.0001
