[2023-07-20 06:40:05,433] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:05,433] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:05,433] torch._dynamo.eval_frame: [DEBUG] skipping helper /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:05,433] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:05,433] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:05,433] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:05,434] torch._utils_internal: [INFO] dynamo _convert_frame_assert._compile: {'co_name': 'func', 'co_filename': 'slide/examples/target_1_add_func.py', 'co_firstlineno': 25, 'cache_size': 0}
[2023-07-20 06:40:05,434] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func slide/examples/target_1_add_func.py:25
[2023-07-20 06:40:05,434] torch.fx.experimental.symbolic_shapes: [INFO] 0.0: create_env
[2023-07-20 06:40:05,434] torch._subclasses.fake_tensor: [DEBUG] create_mode 0x7fa1aa5bfb80
TRACE starts_line func slide/examples/target_1_add_func.py:25
    @dynamo.optimize(my_compiler)
[2023-07-20 06:40:05,460] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['x'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:05,461] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['y'] (10,) [<DimDynamic.STATIC: 2>] [None]
TRACE starts_line func slide/examples/target_1_add_func.py:27
        xx = x - x
[2023-07-20 06:40:05,461] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x []
[2023-07-20 06:40:05,461] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x [TensorVariable()]
[2023-07-20 06:40:05,461] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBTRACT None [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:05,462] torch._subclasses.fake_tensor: [DEBUG] FakeTensorMode.__torch_dispatch__: aten.sub.Tensor
[2023-07-20 06:40:05,463] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST xx [TensorVariable()]
TRACE starts_line func slide/examples/target_1_add_func.py:28
        c = add(xx, y)
[2023-07-20 06:40:05,463] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL add []
[2023-07-20 06:40:05,464] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST xx [UserFunctionVariable()]
[2023-07-20 06:40:05,464] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST y [UserFunctionVariable(), TensorVariable()]
[2023-07-20 06:40:05,464] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [UserFunctionVariable(), TensorVariable(), TensorVariable()]
[2023-07-20 06:40:05,464] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object add at 0x7fa1e1814f50, file "slide/examples/target_1_add_func.py", line 20>
TRACE starts_line add slide/examples/target_1_add_func.py:20 (inline depth: 1)
    def add(a, b):
TRACE starts_line add slide/examples/target_1_add_func.py:21 (inline depth: 1)
        c = a + b
[2023-07-20 06:40:05,464] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []
[2023-07-20 06:40:05,464] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]
[2023-07-20 06:40:05,464] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:05,465] torch._subclasses.fake_tensor: [DEBUG] FakeTensorMode.__torch_dispatch__: aten.add.Tensor
[2023-07-20 06:40:05,465] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST c [TensorVariable()]
TRACE starts_line add slide/examples/target_1_add_func.py:22 (inline depth: 1)
        return c
[2023-07-20 06:40:05,465] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST c []
[2023-07-20 06:40:05,465] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2023-07-20 06:40:05,465] torch._dynamo.symbolic_convert: [DEBUG] DONE INLINING <code object add at 0x7fa1e1814f50, file "slide/examples/target_1_add_func.py", line 20>
[2023-07-20 06:40:05,465] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST c [TensorVariable()]
TRACE starts_line func slide/examples/target_1_add_func.py:29
        return c
[2023-07-20 06:40:05,466] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST c []
[2023-07-20 06:40:05,466] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2023-07-20 06:40:05,466] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing func (RETURN_VALUE)
[2023-07-20 06:40:05,466] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile
[2023-07-20 06:40:05,466] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file slide/examples/target_1_add_func.py, line 29 in func>], graph_break=False)
TRACED GRAPH
 ===== __compiled_fn_0 =====
 <eval_with_key>.0 class GraphModule(torch.nn.Module):
    def forward(self, L_x_ : torch.Tensor, L_y_ : torch.Tensor):
        l_x_ = L_x_
        l_y_ = L_y_
        
        # File: slide/examples/target_1_add_func.py:27, code: xx = x - x
        sub = l_x_ - l_x_;  l_x_ = None
        
        # File: slide/examples/target_1_add_func.py:21, code: c = a + b
        add = sub + l_y_;  sub = l_y_ = None
        return (add,)
        

TRACED GRAPH
 __compiled_fn_0 <eval_with_key>.0 opcode         name    target                   args          kwargs
-------------  ------  -----------------------  ------------  --------
placeholder    l_x_    L_x_                     ()            {}
placeholder    l_y_    L_y_                     ()            {}
call_function  sub     <built-in function sub>  (l_x_, l_x_)  {}
call_function  add     <built-in function add>  (sub, l_y_)   {}
output         output  output                   ((add,),)     {}

TRACED GRAPH TENSOR SIZES
===== __compiled_fn_0 =====
l_x_: (10,)
l_y_: (10,)
sub: (10,)
add: (10,)

[2023-07-20 06:40:05,469] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler
[2023-07-20 06:40:05,470] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler
ORIGINAL BYTECODE func slide/examples/target_1_add_func.py line 25 
 27           0 LOAD_FAST                0 (x)
              2 LOAD_FAST                0 (x)
              4 BINARY_SUBTRACT
              6 STORE_FAST               2 (xx)

 28           8 LOAD_GLOBAL              0 (add)
             10 LOAD_FAST                2 (xx)
             12 LOAD_FAST                1 (y)
             14 CALL_FUNCTION            2
             16 STORE_FAST               3 (c)

 29          18 LOAD_FAST                3 (c)
             20 RETURN_VALUE


MODIFIED BYTECODE func slide/examples/target_1_add_func.py line 25 
 25           0 LOAD_GLOBAL              1 (__compiled_fn_0)
              2 LOAD_FAST                0 (x)
              4 LOAD_FAST                1 (y)
              6 CALL_FUNCTION            2
              8 UNPACK_SEQUENCE          1
             10 RETURN_VALUE


[2023-07-20 06:40:05,491] torch.fx.experimental.symbolic_shapes: [INFO] 0.0: produce_guards
[2023-07-20 06:40:05,491] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['x'].size()[0] == 10
[2023-07-20 06:40:05,491] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['x'].stride()[0] == 1
[2023-07-20 06:40:05,491] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['x'].storage_offset() == 0
[2023-07-20 06:40:05,491] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['y'].size()[0] == 10
[2023-07-20 06:40:05,491] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['y'].stride()[0] == 1
[2023-07-20 06:40:05,491] torch.fx.experimental.symbolic_shapes: [DEBUG] 0.0: Skipping guard L['y'].storage_offset() == 0
[2023-07-20 06:40:05,491] torch._utils_internal: [INFO] dynamic produce_guards: {'co_name': 'func', 'co_filename': 'slide/examples/target_1_add_func.py', 'co_firstlineno': 25, 'num_guards': 0, 'free_symbols': 0}
GUARDS:
  hasattr(L['x'], '_dynamo_dynamic_indices') == False
  hasattr(L['y'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:05,494] torch._dynamo.eval_frame: [DEBUG] skipping _fn /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:05,494] torch._dynamo.eval_frame: [DEBUG] skipping nothing /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:05,494] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:05,494] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
my_compiler() called with FX graph:
res: tensor([-0.9101,  1.6782,  0.2209, -0.4828,  0.1415, -0.7252,  2.4315,  1.1565,
        -0.5430,  0.9494])
[2023-07-20 06:40:05,495] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[2023-07-20 06:40:05,495] torch._dynamo.utils: [INFO] Function                          Runtimes (s)
[2023-07-20 06:40:05,495] torch._dynamo.utils: [INFO] ------------------------------  --------------
[2023-07-20 06:40:05,495] torch._dynamo.utils: [INFO] _compile                                0.0598
[2023-07-20 06:40:05,495] torch._dynamo.utils: [INFO] OutputGraph.call_user_compiler          0.0001
