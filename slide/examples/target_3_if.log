[2023-07-20 06:40:11,281] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:11,281] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:11,281] torch._dynamo.eval_frame: [DEBUG] skipping helper /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:11,281] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:11,281] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:11,281] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:11,281] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing toy_example slide/examples/target_3_if.py:18
TRACE starts_line toy_example slide/examples/target_3_if.py:18
    @torch.compile(backend=my_compiler)
[2023-07-20 06:40:11,307] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['a'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:11,308] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b'] (10,) [<DimDynamic.STATIC: 2>] [None]
TRACE starts_line toy_example slide/examples/target_3_if.py:20
        x = a / (torch.abs(a) + 1)
[2023-07-20 06:40:11,308] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []
[2023-07-20 06:40:11,308] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch [TensorVariable()]
[2023-07-20 06:40:11,308] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR abs [TensorVariable(), TorchVariable(<module 'torch' from '/repos/gglin001/pytorch/torch/__init__.py'>)]
[2023-07-20 06:40:11,309] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [TensorVariable(), TorchVariable(<built-in method abs of type object at 0x7f5f3b047560>)]
[2023-07-20 06:40:11,309] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [TensorVariable(), TorchVariable(<built-in method abs of type object at 0x7f5f3b047560>), TensorVariable()]
[2023-07-20 06:40:11,317] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 1 [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:11,317] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable(), ConstantVariable(int)]
[2023-07-20 06:40:11,318] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_TRUE_DIVIDE None [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:11,319] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST x [TensorVariable()]
TRACE starts_line toy_example slide/examples/target_3_if.py:21
        if b.sum() < 0:
[2023-07-20 06:40:11,319] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b []
[2023-07-20 06:40:11,319] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]
[2023-07-20 06:40:11,319] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]
[2023-07-20 06:40:11,320] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]
[2023-07-20 06:40:11,320] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP < [TensorVariable(), ConstantVariable(int)]
[2023-07-20 06:40:11,321] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 38 [TensorVariable()]
[2023-07-20 06:40:11,321] torch._dynamo.symbolic_convert: [DEBUG] generic_jump triggered compile
[2023-07-20 06:40:11,321] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file slide/examples/target_3_if.py, line 21 in toy_example>], graph_break=True)
TRACED GRAPH
 ===== __compiled_fn_0 =====
 <eval_with_key>.0 class GraphModule(torch.nn.Module):
    def forward(self, L_a_ : torch.Tensor, L_b_ : torch.Tensor):
        l_a_ = L_a_
        l_b_ = L_b_
        
        # File: slide/examples/target_3_if.py:20, code: x = a / (torch.abs(a) + 1)
        abs_1 = torch.abs(l_a_)
        add = abs_1 + 1;  abs_1 = None
        truediv = l_a_ / add;  l_a_ = add = None
        
        # File: slide/examples/target_3_if.py:21, code: if b.sum() < 0:
        sum_1 = l_b_.sum();  l_b_ = None
        lt = sum_1 < 0;  sum_1 = None
        return (truediv, lt)
        

TRACED GRAPH
 __compiled_fn_0 <eval_with_key>.0 opcode         name     target                                                  args              kwargs
-------------  -------  ------------------------------------------------------  ----------------  --------
placeholder    l_a_     L_a_                                                    ()                {}
placeholder    l_b_     L_b_                                                    ()                {}
call_function  abs_1    <built-in method abs of type object at 0x7f5f3b047560>  (l_a_,)           {}
call_function  add      <built-in function add>                                 (abs_1, 1)        {}
call_function  truediv  <built-in function truediv>                             (l_a_, add)       {}
call_method    sum_1    sum                                                     (l_b_,)           {}
call_function  lt       <built-in function lt>                                  (sum_1, 0)        {}
output         output   output                                                  ((truediv, lt),)  {}

TRACED GRAPH TENSOR SIZES
===== __compiled_fn_0 =====
l_a_: (10,)
l_b_: (10,)
abs_1: (10,)
add: (10,)
truediv: (10,)
sum_1: ()
lt: ()

[2023-07-20 06:40:11,325] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler
[2023-07-20 06:40:11,325] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler
ORIGINAL BYTECODE toy_example slide/examples/target_3_if.py line 18 
 20           0 LOAD_FAST                0 (a)
              2 LOAD_GLOBAL              0 (torch)
              4 LOAD_METHOD              1 (abs)
              6 LOAD_FAST                0 (a)
              8 CALL_METHOD              1
             10 LOAD_CONST               1 (1)
             12 BINARY_ADD
             14 BINARY_TRUE_DIVIDE
             16 STORE_FAST               2 (x)

 21          18 LOAD_FAST                1 (b)
             20 LOAD_METHOD              2 (sum)
             22 CALL_METHOD              0
             24 LOAD_CONST               2 (0)
             26 COMPARE_OP               0 (<)
             28 POP_JUMP_IF_FALSE       38

 22          30 LOAD_FAST                1 (b)
             32 LOAD_CONST               3 (-1)
             34 BINARY_MULTIPLY
             36 STORE_FAST               1 (b)

 23     >>   38 LOAD_FAST                2 (x)
             40 LOAD_FAST                1 (b)
             42 BINARY_MULTIPLY
             44 RETURN_VALUE


MODIFIED BYTECODE toy_example slide/examples/target_3_if.py line 18 
 18           0 LOAD_GLOBAL              3 (__compiled_fn_0)
              2 LOAD_FAST                0 (a)
              4 LOAD_FAST                1 (b)
              6 CALL_FUNCTION            2
              8 UNPACK_SEQUENCE          2
             10 STORE_FAST               2 (x)
             12 POP_JUMP_IF_FALSE       24
             14 LOAD_GLOBAL              4 (__resume_at_30_1)
             16 LOAD_FAST                1 (b)
             18 LOAD_FAST                2 (x)
             20 CALL_FUNCTION            2
             22 RETURN_VALUE
        >>   24 LOAD_GLOBAL              5 (__resume_at_38_2)
             26 LOAD_FAST                1 (b)
             28 LOAD_FAST                2 (x)
             30 CALL_FUNCTION            2
             32 RETURN_VALUE


GUARDS:
  hasattr(L['a'], '_dynamo_dynamic_indices') == False
  hasattr(L['b'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:11,351] torch._dynamo.eval_frame: [DEBUG] skipping _fn /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:11,351] torch._dynamo.eval_frame: [DEBUG] skipping nothing /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:11,351] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:11,351] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:11,351] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in toy_example> slide/examples/target_3_if.py:21
TRACE starts_line <resume in toy_example> slide/examples/target_3_if.py:21
        if b.sum() < 0:
[2023-07-20 06:40:11,351] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:11,352] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['x'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:11,352] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 40 []
TRACE starts_line <resume in toy_example> slide/examples/target_3_if.py:23
        return x * b
[2023-07-20 06:40:11,352] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x []
[2023-07-20 06:40:11,352] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]
[2023-07-20 06:40:11,352] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_MULTIPLY None [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:11,353] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2023-07-20 06:40:11,353] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in toy_example> (RETURN_VALUE)
[2023-07-20 06:40:11,353] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile
[2023-07-20 06:40:11,353] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file slide/examples/target_3_if.py, line 23 in <resume in toy_example>>], graph_break=False)
TRACED GRAPH
 ===== __compiled_fn_3 =====
 <eval_with_key>.1 class GraphModule(torch.nn.Module):
    def forward(self, L_b_ : torch.Tensor, L_x_ : torch.Tensor):
        l_b_ = L_b_
        l_x_ = L_x_
        
        # File: slide/examples/target_3_if.py:23, code: return x * b
        mul = l_x_ * l_b_;  l_x_ = l_b_ = None
        return (mul,)
        

TRACED GRAPH
 __compiled_fn_3 <eval_with_key>.1 opcode         name    target                   args          kwargs
-------------  ------  -----------------------  ------------  --------
placeholder    l_b_    L_b_                     ()            {}
placeholder    l_x_    L_x_                     ()            {}
call_function  mul     <built-in function mul>  (l_x_, l_b_)  {}
output         output  output                   ((mul,),)     {}

TRACED GRAPH TENSOR SIZES
===== __compiled_fn_3 =====
l_b_: (10,)
l_x_: (10,)
mul: (10,)

[2023-07-20 06:40:11,354] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler
[2023-07-20 06:40:11,354] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler
ORIGINAL BYTECODE <resume in toy_example> slide/examples/target_3_if.py line 21 
 21           0 JUMP_ABSOLUTE           40
              2 LOAD_FAST                2 (a)
              4 LOAD_GLOBAL              0 (torch)
              6 LOAD_ATTR                1 (abs)
              8 LOAD_FAST                2 (a)
             10 CALL_FUNCTION            1
             12 LOAD_CONST               1 (1)
             14 BINARY_ADD
             16 BINARY_TRUE_DIVIDE
             18 STORE_FAST               1 (x)
             20 LOAD_FAST                0 (b)
             22 LOAD_ATTR                2 (sum)
             24 CALL_FUNCTION            0
             26 LOAD_CONST               2 (0)
             28 COMPARE_OP               0 (<)
             30 POP_JUMP_IF_FALSE       40
             32 LOAD_FAST                0 (b)
             34 LOAD_CONST               3 (-1)
             36 BINARY_MULTIPLY
             38 STORE_FAST               0 (b)

 23     >>   40 LOAD_FAST                1 (x)
             42 LOAD_FAST                0 (b)
             44 BINARY_MULTIPLY
             46 RETURN_VALUE


MODIFIED BYTECODE <resume in toy_example> slide/examples/target_3_if.py line 21 
 21           0 LOAD_GLOBAL              3 (__compiled_fn_3)
              2 LOAD_FAST                0 (b)
              4 LOAD_FAST                1 (x)
              6 CALL_FUNCTION            2
              8 UNPACK_SEQUENCE          1
             10 RETURN_VALUE


GUARDS:
  hasattr(L['b'], '_dynamo_dynamic_indices') == False
  hasattr(L['x'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:11,356] torch._dynamo.eval_frame: [DEBUG] skipping __call__ /opt/conda/envs/pyenv/lib/python3.8/weakref.py
[2023-07-20 06:40:11,356] torch._dynamo.eval_frame: [DEBUG] skipping del_ten /repos/gglin001/pytorch/torch/_subclasses/meta_utils.py
[2023-07-20 06:40:11,356] torch._dynamo.eval_frame: [DEBUG] skipping pop /opt/conda/envs/pyenv/lib/python3.8/weakref.py
[2023-07-20 06:40:11,356] torch._dynamo.eval_frame: [DEBUG] skipping __hash__ /repos/gglin001/pytorch/torch/utils/weak.py
[2023-07-20 06:40:11,356] torch._dynamo.eval_frame: [DEBUG] skipping expired /repos/gglin001/pytorch/torch/multiprocessing/reductions.py
[2023-07-20 06:40:11,356] torch._dynamo.eval_frame: [DEBUG] skipping _expired /repos/gglin001/pytorch/torch/storage.py
my_compiler() called with FX graph:
my_compiler() called with FX graph:
torch.Size([10])
[2023-07-20 06:40:11,357] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[2023-07-20 06:40:11,357] torch._dynamo.utils: [INFO] Function                          Runtimes (s)
[2023-07-20 06:40:11,357] torch._dynamo.utils: [INFO] ------------------------------  --------------
[2023-07-20 06:40:11,357] torch._dynamo.utils: [INFO] _compile                                0.0746
[2023-07-20 06:40:11,357] torch._dynamo.utils: [INFO] OutputGraph.call_user_compiler          0.0002
