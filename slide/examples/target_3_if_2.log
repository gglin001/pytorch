[2023-07-20 06:40:09,808] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:09,809] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:09,809] torch._dynamo.eval_frame: [DEBUG] skipping helper /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:09,809] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:09,809] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:09,809] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:09,809] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing toy_example slide/examples/target_3_if_2.py:24
TRACE starts_line toy_example slide/examples/target_3_if_2.py:24
    @torch.compile(backend=my_compiler)
[2023-07-20 06:40:09,834] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['a'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:09,835] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b'] (10,) [<DimDynamic.STATIC: 2>] [None]
TRACE starts_line toy_example slide/examples/target_3_if_2.py:26
        x = a / (torch.abs(a) + 1)
[2023-07-20 06:40:09,836] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []
[2023-07-20 06:40:09,836] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch [TensorVariable()]
[2023-07-20 06:40:09,836] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR abs [TensorVariable(), TorchVariable(<module 'torch' from '/repos/gglin001/pytorch/torch/__init__.py'>)]
[2023-07-20 06:40:09,836] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [TensorVariable(), TorchVariable(<built-in method abs of type object at 0x7fafeec47560>)]
[2023-07-20 06:40:09,836] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [TensorVariable(), TorchVariable(<built-in method abs of type object at 0x7fafeec47560>), TensorVariable()]
[2023-07-20 06:40:09,845] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 1 [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:09,845] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable(), ConstantVariable(int)]
[2023-07-20 06:40:09,846] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_TRUE_DIVIDE None [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:09,846] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST x [TensorVariable()]
TRACE starts_line toy_example slide/examples/target_3_if_2.py:27
        b = if_func(b)
[2023-07-20 06:40:09,846] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL if_func []
[2023-07-20 06:40:09,847] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [UserFunctionVariable()]
[2023-07-20 06:40:09,847] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [UserFunctionVariable(), TensorVariable()]
[2023-07-20 06:40:09,847] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object if_func at 0x7fafef13df50, file "slide/examples/target_3_if_2.py", line 18>
TRACE starts_line if_func slide/examples/target_3_if_2.py:18 (inline depth: 1)
    def if_func(b):
TRACE starts_line if_func slide/examples/target_3_if_2.py:19 (inline depth: 1)
        if b.sum() < 0:
[2023-07-20 06:40:09,847] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b []
[2023-07-20 06:40:09,847] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]
[2023-07-20 06:40:09,848] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]
[2023-07-20 06:40:09,848] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]
[2023-07-20 06:40:09,848] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP < [TensorVariable(), ConstantVariable(int)]
[2023-07-20 06:40:09,849] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 20 [TensorVariable()]
[2023-07-20 06:40:09,849] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object if_func at 0x7fafef13df50, file "slide/examples/target_3_if_2.py", line 18>
[2023-07-20 06:40:09,849] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 2 nodes
[2023-07-20 06:40:09,850] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile
Graph break: Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow from user code at   File "slide/examples/target_3_if_2.py", line 27, in toy_example
    b = if_func(b)
  File "slide/examples/target_3_if_2.py", line 19, in if_func
    if b.sum() < 0:

[2023-07-20 06:40:09,850] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes
[2023-07-20 06:40:09,850] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow', user_stack=[<FrameSummary file slide/examples/target_3_if_2.py, line 27 in toy_example>, <FrameSummary file slide/examples/target_3_if_2.py, line 19 in if_func>], graph_break=True)
[2023-07-20 06:40:09,850] torch._dynamo.output_graph: [DEBUG] REMOVE UNUSED GRAPHARG L['b']
TRACED GRAPH
 ===== __compiled_fn_0 =====
 <eval_with_key>.0 class GraphModule(torch.nn.Module):
    def forward(self, L_a_ : torch.Tensor):
        l_a_ = L_a_
        
        # File: slide/examples/target_3_if_2.py:26, code: x = a / (torch.abs(a) + 1)
        abs_1 = torch.abs(l_a_)
        add = abs_1 + 1;  abs_1 = None
        truediv = l_a_ / add;  l_a_ = add = None
        return (truediv,)
        

TRACED GRAPH
 __compiled_fn_0 <eval_with_key>.0 opcode         name     target                                                  args           kwargs
-------------  -------  ------------------------------------------------------  -------------  --------
placeholder    l_a_     L_a_                                                    ()             {}
call_function  abs_1    <built-in method abs of type object at 0x7fafeec47560>  (l_a_,)        {}
call_function  add      <built-in function add>                                 (abs_1, 1)     {}
call_function  truediv  <built-in function truediv>                             (l_a_, add)    {}
output         output   output                                                  ((truediv,),)  {}

TRACED GRAPH TENSOR SIZES
===== __compiled_fn_0 =====
l_a_: (10,)
abs_1: (10,)
add: (10,)
truediv: (10,)

[2023-07-20 06:40:09,854] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler
[2023-07-20 06:40:09,854] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler
ORIGINAL BYTECODE toy_example slide/examples/target_3_if_2.py line 24 
 26           0 LOAD_FAST                0 (a)
              2 LOAD_GLOBAL              0 (torch)
              4 LOAD_METHOD              1 (abs)
              6 LOAD_FAST                0 (a)
              8 CALL_METHOD              1
             10 LOAD_CONST               1 (1)
             12 BINARY_ADD
             14 BINARY_TRUE_DIVIDE
             16 STORE_FAST               2 (x)

 27          18 LOAD_GLOBAL              2 (if_func)
             20 LOAD_FAST                1 (b)
             22 CALL_FUNCTION            1
             24 STORE_FAST               1 (b)

 28          26 LOAD_FAST                2 (x)
             28 LOAD_FAST                1 (b)
             30 BINARY_MULTIPLY
             32 RETURN_VALUE


MODIFIED BYTECODE toy_example slide/examples/target_3_if_2.py line 24 
 24           0 LOAD_GLOBAL              3 (__compiled_fn_0)
              2 LOAD_FAST                0 (a)
              4 CALL_FUNCTION            1
              6 STORE_FAST               3 (___graph_out_0)
              8 LOAD_GLOBAL              2 (if_func)
             10 LOAD_FAST                1 (b)
             12 LOAD_FAST                3 (___graph_out_0)
             14 LOAD_CONST               2 (0)
             16 BINARY_SUBSCR
             18 STORE_FAST               2 (x)

 27          20 CALL_FUNCTION            1
             22 LOAD_GLOBAL              4 (__resume_at_24_1)
             24 ROT_TWO
             26 LOAD_FAST                2 (x)
             28 CALL_FUNCTION            2
             30 RETURN_VALUE


GUARDS:
  hasattr(L['a'], '_dynamo_dynamic_indices') == False
  hasattr(L['b'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:09,879] torch._dynamo.eval_frame: [DEBUG] skipping _fn /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:09,879] torch._dynamo.eval_frame: [DEBUG] skipping nothing /repos/gglin001/pytorch/torch/_dynamo/eval_frame.py
[2023-07-20 06:40:09,879] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:09,879] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /opt/conda/envs/pyenv/lib/python3.8/contextlib.py
[2023-07-20 06:40:09,880] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing if_func slide/examples/target_3_if_2.py:18
TRACE starts_line if_func slide/examples/target_3_if_2.py:18
    def if_func(b):
[2023-07-20 06:40:09,880] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b'] (10,) [<DimDynamic.STATIC: 2>] [None]
TRACE starts_line if_func slide/examples/target_3_if_2.py:19
        if b.sum() < 0:
[2023-07-20 06:40:09,880] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b []
[2023-07-20 06:40:09,880] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]
[2023-07-20 06:40:09,881] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]
[2023-07-20 06:40:09,881] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]
[2023-07-20 06:40:09,881] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP < [TensorVariable(), ConstantVariable(int)]
[2023-07-20 06:40:09,882] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 20 [TensorVariable()]
[2023-07-20 06:40:09,882] torch._dynamo.symbolic_convert: [DEBUG] generic_jump triggered compile
[2023-07-20 06:40:09,882] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file slide/examples/target_3_if_2.py, line 19 in if_func>], graph_break=True)
TRACED GRAPH
 ===== __compiled_fn_2 =====
 <eval_with_key>.1 class GraphModule(torch.nn.Module):
    def forward(self, L_b_ : torch.Tensor):
        l_b_ = L_b_
        
        # File: slide/examples/target_3_if_2.py:19, code: if b.sum() < 0:
        sum_1 = l_b_.sum();  l_b_ = None
        lt = sum_1 < 0;  sum_1 = None
        return (lt,)
        

TRACED GRAPH
 __compiled_fn_2 <eval_with_key>.1 opcode         name    target                  args        kwargs
-------------  ------  ----------------------  ----------  --------
placeholder    l_b_    L_b_                    ()          {}
call_method    sum_1   sum                     (l_b_,)     {}
call_function  lt      <built-in function lt>  (sum_1, 0)  {}
output         output  output                  ((lt,),)    {}

TRACED GRAPH TENSOR SIZES
===== __compiled_fn_2 =====
l_b_: (10,)
sum_1: ()
lt: ()

[2023-07-20 06:40:09,883] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler
[2023-07-20 06:40:09,883] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler
ORIGINAL BYTECODE if_func slide/examples/target_3_if_2.py line 18 
 19           0 LOAD_FAST                0 (b)
              2 LOAD_METHOD              0 (sum)
              4 CALL_METHOD              0
              6 LOAD_CONST               1 (0)
              8 COMPARE_OP               0 (<)
             10 POP_JUMP_IF_FALSE       20

 20          12 LOAD_FAST                0 (b)
             14 LOAD_CONST               2 (-1)
             16 BINARY_MULTIPLY
             18 STORE_FAST               0 (b)

 21     >>   20 LOAD_FAST                0 (b)
             22 RETURN_VALUE


MODIFIED BYTECODE if_func slide/examples/target_3_if_2.py line 18 
 18           0 LOAD_GLOBAL              1 (__compiled_fn_2)
              2 LOAD_FAST                0 (b)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 POP_JUMP_IF_FALSE       18
             10 LOAD_GLOBAL              2 (__resume_at_12_3)
             12 LOAD_FAST                0 (b)
             14 CALL_FUNCTION            1
             16 RETURN_VALUE
        >>   18 LOAD_GLOBAL              3 (__resume_at_20_4)
             20 LOAD_FAST                0 (b)
             22 CALL_FUNCTION            1
             24 RETURN_VALUE


GUARDS:
  hasattr(L['b'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:09,885] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in if_func> slide/examples/target_3_if_2.py:19
TRACE starts_line <resume in if_func> slide/examples/target_3_if_2.py:19
        if b.sum() < 0:
[2023-07-20 06:40:09,886] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['b'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:09,886] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 22 []
TRACE starts_line <resume in if_func> slide/examples/target_3_if_2.py:21
        return b
[2023-07-20 06:40:09,886] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b []
[2023-07-20 06:40:09,886] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2023-07-20 06:40:09,886] torch._dynamo.convert_frame: [DEBUG] Skipping frame because no content in function call <resume in if_func>                     slide/examples/target_3_if_2.py 19
[2023-07-20 06:40:09,887] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in toy_example> slide/examples/target_3_if_2.py:27
TRACE starts_line <resume in toy_example> slide/examples/target_3_if_2.py:27
        b = if_func(b)
[2023-07-20 06:40:09,887] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['___stack0'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:09,887] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['x'] (10,) [<DimDynamic.STATIC: 2>] [None]
[2023-07-20 06:40:09,888] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []
[2023-07-20 06:40:09,888] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 28 [TensorVariable()]
[2023-07-20 06:40:09,888] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST b [TensorVariable()]
TRACE starts_line <resume in toy_example> slide/examples/target_3_if_2.py:28
        return x * b
[2023-07-20 06:40:09,888] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x []
[2023-07-20 06:40:09,888] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]
[2023-07-20 06:40:09,888] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_MULTIPLY None [TensorVariable(), TensorVariable()]
[2023-07-20 06:40:09,889] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2023-07-20 06:40:09,889] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in toy_example> (RETURN_VALUE)
[2023-07-20 06:40:09,889] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile
[2023-07-20 06:40:09,889] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file slide/examples/target_3_if_2.py, line 28 in <resume in toy_example>>], graph_break=False)
TRACED GRAPH
 ===== __compiled_fn_5 =====
 <eval_with_key>.2 class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_ : torch.Tensor, L_x_ : torch.Tensor):
        l_stack0_ = L_stack0_
        l_x_ = L_x_
        
        # File: slide/examples/target_3_if_2.py:28, code: return x * b
        mul = l_x_ * l_stack0_;  l_x_ = l_stack0_ = None
        return (mul,)
        

TRACED GRAPH
 __compiled_fn_5 <eval_with_key>.2 opcode         name       target                   args               kwargs
-------------  ---------  -----------------------  -----------------  --------
placeholder    l_stack0_  L_stack0_                ()                 {}
placeholder    l_x_       L_x_                     ()                 {}
call_function  mul        <built-in function mul>  (l_x_, l_stack0_)  {}
output         output     output                   ((mul,),)          {}

TRACED GRAPH TENSOR SIZES
===== __compiled_fn_5 =====
l_stack0_: (10,)
l_x_: (10,)
mul: (10,)

[2023-07-20 06:40:09,890] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler
[2023-07-20 06:40:09,890] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler
ORIGINAL BYTECODE <resume in toy_example> slide/examples/target_3_if_2.py line 27 
 27           0 LOAD_FAST                0 (___stack0)
              2 JUMP_ABSOLUTE           28
              4 LOAD_FAST                2 (a)
              6 LOAD_GLOBAL              0 (torch)
              8 LOAD_ATTR                1 (abs)
             10 LOAD_FAST                2 (a)
             12 CALL_FUNCTION            1
             14 LOAD_CONST               1 (1)
             16 BINARY_ADD
             18 BINARY_TRUE_DIVIDE
             20 STORE_FAST               1 (x)
             22 LOAD_GLOBAL              2 (if_func)
             24 LOAD_FAST                3 (b)
             26 CALL_FUNCTION            1
        >>   28 STORE_FAST               3 (b)

 28          30 LOAD_FAST                1 (x)
             32 LOAD_FAST                3 (b)
             34 BINARY_MULTIPLY
             36 RETURN_VALUE


MODIFIED BYTECODE <resume in toy_example> slide/examples/target_3_if_2.py line 27 
 27           0 LOAD_GLOBAL              3 (__compiled_fn_5)
              2 LOAD_FAST                0 (___stack0)
              4 LOAD_FAST                1 (x)
              6 CALL_FUNCTION            2
              8 UNPACK_SEQUENCE          1
             10 RETURN_VALUE


GUARDS:
  hasattr(L['x'], '_dynamo_dynamic_indices') == False
  hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False
  ___is_grad_enabled()
  not ___are_deterministic_algorithms_enabled()
  ___is_torch_function_enabled()
  utils_device.CURRENT_DEVICE == None
[2023-07-20 06:40:09,892] torch._dynamo.eval_frame: [DEBUG] skipping __call__ /opt/conda/envs/pyenv/lib/python3.8/weakref.py
[2023-07-20 06:40:09,892] torch._dynamo.eval_frame: [DEBUG] skipping del_ten /repos/gglin001/pytorch/torch/_subclasses/meta_utils.py
[2023-07-20 06:40:09,892] torch._dynamo.eval_frame: [DEBUG] skipping pop /opt/conda/envs/pyenv/lib/python3.8/weakref.py
[2023-07-20 06:40:09,892] torch._dynamo.eval_frame: [DEBUG] skipping __hash__ /repos/gglin001/pytorch/torch/utils/weak.py
[2023-07-20 06:40:09,892] torch._dynamo.eval_frame: [DEBUG] skipping expired /repos/gglin001/pytorch/torch/multiprocessing/reductions.py
[2023-07-20 06:40:09,892] torch._dynamo.eval_frame: [DEBUG] skipping _expired /repos/gglin001/pytorch/torch/storage.py
my_compiler() called with FX graph:
my_compiler() called with FX graph:
my_compiler() called with FX graph:
torch.Size([10])
[2023-07-20 06:40:09,892] torch._dynamo.utils: [INFO] TorchDynamo compilation metrics:
[2023-07-20 06:40:09,892] torch._dynamo.utils: [INFO] Function                          Runtimes (s)
[2023-07-20 06:40:09,892] torch._dynamo.utils: [INFO] ------------------------------  --------------
[2023-07-20 06:40:09,892] torch._dynamo.utils: [INFO] _compile                                0.0821
[2023-07-20 06:40:09,892] torch._dynamo.utils: [INFO] OutputGraph.call_user_compiler          0.0003
